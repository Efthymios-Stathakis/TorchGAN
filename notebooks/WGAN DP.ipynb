{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST, FashionMNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from models.vis_utils import show_img_batch\n",
    "%matplotlib inline\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "mnist = MNIST(root=\"../datasets/\", download=False, transform=transform)\n",
    "mnist_dl = DataLoader(mnist, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenConvTransposeBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, final=False):\n",
    "        super().__init__()\n",
    "        if not final:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=in_channels, \n",
    "                                   out_channels=out_channels, \n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=stride),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels=in_channels, \n",
    "                                   out_channels=out_channels, \n",
    "                                   kernel_size= kernel_size, \n",
    "                                   stride=stride),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.gen = nn.Sequential(\n",
    "            GenConvTransposeBlock(z_dim, hidden_dim * 4),\n",
    "            GenConvTransposeBlock(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            GenConvTransposeBlock(hidden_dim * 2, hidden_dim * 1),\n",
    "            GenConvTransposeBlock(hidden_dim * 1, im_chan,  kernel_size=4, final=True)\n",
    "        )\n",
    "\n",
    "    def unsqueeze_noise(self, x):\n",
    "        return x.view(len(x), self.z_dim, 1, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.unsqueeze_noise(x)\n",
    "        return self.gen(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(n_samples, z_dim, device):\n",
    "    return torch.randn((n_samples, z_dim), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscConvBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=4, stride=2, final=False):\n",
    "        super().__init__()\n",
    "        if not final:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, \n",
    "                          out_channels=out_channels, \n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels, \n",
    "                          out_channels=out_channels, \n",
    "                          kernel_size= kernel_size, \n",
    "                          stride=stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.crit = nn.Sequential(\n",
    "            DiscConvBlock(im_chan, hidden_dim * 1),\n",
    "            DiscConvBlock(hidden_dim * 1, hidden_dim * 2),\n",
    "            DiscConvBlock(hidden_dim * 2, 1, final=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.crit(x).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 128\n",
    "z_dim = 64\n",
    "hidden_dim = 64\n",
    "noise_input = get_noise(n_samples, z_dim, device)\n",
    "real = next(iter(mnist_dl))[0].to(device)\n",
    "gen = Generator(z_dim=z_dim, im_chan=1, hidden_dim=64).to(device)\n",
    "fake = gen(noise_input)\n",
    "crit = Critic(1, 64).to(device)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "crit = crit.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gradient(crit, real, fake, epsilon):\n",
    "\n",
    "    mixed = (1-epsilon) * fake + epsilon * real\n",
    "    score = crit(mixed)\n",
    "    return torch.autograd.grad(\n",
    "         inputs = mixed, \n",
    "         outputs = score, \n",
    "         grad_outputs=torch.ones_like(score), \n",
    "         retain_graph=True, \n",
    "         create_graph=True\n",
    "         )[0]\n",
    "\n",
    "def test_get_gradient(image_shape):\n",
    "    real = torch.randn(*image_shape, device=device) + 1\n",
    "    fake = torch.randn(*image_shape, device=device) - 1\n",
    "    epsilon_shape = [1 for _ in image_shape]\n",
    "    epsilon_shape[0] = image_shape[0]\n",
    "    epsilon = torch.rand(epsilon_shape, device=device).requires_grad_()\n",
    "    gradient = get_gradient(crit, real, fake, epsilon)\n",
    "    assert tuple(gradient.shape) == image_shape\n",
    "    assert gradient.max() > 0\n",
    "    assert gradient.min() < 0\n",
    "    return gradient\n",
    "\n",
    "gradient = test_get_gradient((256, 1, 28, 28))\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(10)\n",
    "image_shape = (4, 1, 28, 28)\n",
    "real = torch.randn(*image_shape, device=device, requires_grad=True) + 1\n",
    "fake = torch.randn(*image_shape, device=device, requires_grad=True) - 1\n",
    "epsilon_shape = [1 for _ in image_shape]\n",
    "epsilon_shape[0] = image_shape[0]\n",
    "epsilon = torch.rand(epsilon_shape, device=device)#.requires_grad_()\n",
    "gradient = get_gradient(crit, real, fake, epsilon)\n",
    "\n",
    "torch.manual_seed(10)\n",
    "real = torch.randn(*image_shape, device=device) + 1\n",
    "fake = torch.randn(*image_shape, device=device) - 1\n",
    "mixed_images = (real * epsilon + fake * (1 - epsilon)).requires_grad_(True)\n",
    "mixed_scores = crit(mixed_images)\n",
    "gradient2 = torch.autograd.grad(inputs=mixed_images, outputs=mixed_scores, grad_outputs=torch.ones_like(mixed_scores))[0]\n",
    "\n",
    "import numpy as np\n",
    "np.all(gradient2.detach().to(\"cpu\").numpy() == gradient.detach().to(\"cpu\").numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(gradient):\n",
    "    '''\n",
    "    Return the gradient penalty, given a gradient.\n",
    "    Given a batch of image gradients, you calculate the magnitude of each image's gradient\n",
    "    and penalize the mean quadratic distance of each magnitude to 1.\n",
    "    Parameters:\n",
    "        gradient: the gradient of the critic's scores, with respect to the mixed image\n",
    "    Returns:\n",
    "        penalty: the gradient penalty\n",
    "    '''\n",
    "    # Flatten the gradients so that each row captures one image\n",
    "    gradient = gradient.view(len(gradient), -1)\n",
    "\n",
    "    # Calculate the magnitude of every row\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    \n",
    "    # Penalize the mean squared distance of the gradient norms from 1\n",
    "    penalty = torch.mean((gradient_norm - 1)**2)\n",
    "    return penalty\n",
    "\n",
    "bad_gradient = torch.zeros(*image_shape)\n",
    "bad_gradient_penalty = gradient_penalty(bad_gradient)\n",
    "assert torch.isclose(bad_gradient_penalty, torch.tensor(1.))\n",
    "\n",
    "image_size = torch.prod(torch.Tensor(image_shape[1:]))\n",
    "good_gradient = torch.ones(*image_shape) / torch.sqrt(image_size)\n",
    "good_gradient_penalty = gradient_penalty(good_gradient)\n",
    "assert torch.isclose(good_gradient_penalty, torch.tensor(0.))\n",
    "\n",
    "random_gradient = test_get_gradient(image_shape)\n",
    "random_gradient_penalty = gradient_penalty(random_gradient)\n",
    "assert torch.abs(random_gradient_penalty - 1) < 0.1\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_epochs = 100\n",
    "z_dim = 64\n",
    "display_step = 2500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "c_lambda = 10\n",
    "crit_repeats = 5\n",
    "device = 'mps' if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the generator and discriminator\n",
    "gen = Generator(z_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(),   lr=lr, betas=(beta_1, beta_2))\n",
    "crit = Critic().to(device) \n",
    "crit_opt = torch.optim.Adam(crit.parameters(), lr=lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "crit = crit.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "cur_step = 0\n",
    "mean_generator_loss = 0\n",
    "mean_critic_loss = 0\n",
    "generator_losses = []\n",
    "critic_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for real, _ in tqdm(mnist_dl):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "\n",
    "        mean_iteration_critic_loss = 0\n",
    "        for k in range(crit_repeats):\n",
    "            ## Update discriminator ##\n",
    "            crit_opt.zero_grad()\n",
    "            \n",
    "            # Get noise corresponding to the current batch_size\n",
    "            noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "                    \n",
    "            fake = gen(noise)\n",
    "            crit_fake = crit(fake.detach())\n",
    "            crit_real = crit(real)\n",
    "            \n",
    "            epsilon = torch.rand(len(fake), fake.shape[1], 1, 1, device=device, requires_grad=True)\n",
    "            gradient = get_gradient(crit, real, fake.detach(), epsilon)\n",
    "            grad_pen = gradient_penalty(gradient)\n",
    "\n",
    "            crit_loss = torch.mean(crit_fake) - torch.mean(crit_real) + c_lambda*grad_pen\n",
    "            mean_iteration_critic_loss += crit_loss.item() / crit_repeats\n",
    "\n",
    "            crit_loss.backward(retain_graph=True)\n",
    "            crit_opt.step()\n",
    "\n",
    "        critic_losses += [mean_iteration_critic_loss]\n",
    "\n",
    "        ### Update generator ###\n",
    "        gen_opt.zero_grad()\n",
    "        noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "                \n",
    "        fake = gen(noise)\n",
    "        gen_fake = crit(fake)\n",
    "        gen_fake_loss = - torch.mean(gen_fake)\n",
    "        gen_fake_loss.backward()\n",
    "        gen_opt.step()        \n",
    "        \n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [gen_fake_loss.item()]\n",
    "\n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            crit_mean = sum(critic_losses[-display_step:]) / display_step\n",
    "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean:.2f}, discriminator loss: {crit_mean:.2f}\")\n",
    "            show_img_batch((fake + 1) / 2)\n",
    "            show_img_batch((real + 1) / 2)\n",
    "            step_bins = 20\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(critic_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Critic Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'gen_state': gen.state_dict(), \n",
    "    'gen_opt_state': gen_opt.state_dict(),\n",
    "    'crit_state': crit.state_dict(),\n",
    "    'crit_opt_state': crit_opt.state_dict()\n",
    "    }, \n",
    "    'wgan_dp.pth'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
